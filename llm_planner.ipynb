{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9431b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea7ec6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models/mistral.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a44ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "_llm_instance = None\n",
    "\n",
    "def get_llm():\n",
    "    global _llm_instance\n",
    "    if _llm_instance is None:\n",
    "        _llm_instance = Llama(\n",
    "            model_path=MODEL_PATH,\n",
    "            n_ctx=4096,\n",
    "            n_threads=8,\n",
    "            n_gpu_layers=0,\n",
    "        )\n",
    "    return _llm_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1b1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_prompt, context):\n",
    "    return f\"\"\"\n",
    "You are a spatial planning engine.\n",
    "\n",
    "Return ONLY valid JSON.\n",
    "No explanation. No markdown.\n",
    "\n",
    "Allowed targets: {context[\"available_targets\"]}\n",
    "\n",
    "Schema:\n",
    "{{\n",
    "  \"object_type\": \"chair\",\n",
    "  \"object_dims\": {{\"width\": float, \"depth\": float}},\n",
    "  \"target\": {{\"name\": string or null}},\n",
    "  \"weights\": {{\"near_target\": float, \"max_clearance\": float, \"near_wall\": float}},\n",
    "  \"constraints\": {{\"min_clearance\": float, \"boundary_margin\": float}},\n",
    "  \"wall_pref\": \"near\" | \"far\" | \"neutral\"\n",
    "}}\n",
    "\n",
    "User request: \"{user_prompt}\"\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d027c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_make_plan_local(user_prompt, context):\n",
    "    llm = get_llm()\n",
    "    prompt = build_prompt(user_prompt, context)\n",
    "\n",
    "    out = llm(prompt, max_tokens=350, temperature=0.1)\n",
    "\n",
    "    text = out[\"choices\"][0][\"text\"]\n",
    "\n",
    "    import re\n",
    "    match = re.search(r'\\{[\\s\\S]*\\}', text)\n",
    "\n",
    "    if not match:\n",
    "        print(\"LLM OUTPUT (not JSON):\\n\", text)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return json.loads(match.group(0))\n",
    "    except:\n",
    "        print(\"Bad JSON:\\n\", match.group(0))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5da349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
